{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import concat_ws, col, udf, expr, monotonically_increasing_id, regexp_replace, split, row_number, lit, array_distinct, explode, lower\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "global stop_words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Movie Recommendation System') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .config(\"spark.executor.cores\", \"16\") \\\n",
    "    .config(\"spark.num.executors\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100000\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Spark configuration for cores and memory\n",
    "print(\"Master:\", spark.sparkContext.master)  # Should print 'local[*]' or 'local[N]'\n",
    "print(\"Executor Memory:\", spark.sparkContext.getConf().get(\"spark.executor.memory\", \"Not Set\"))\n",
    "print(\"Driver Memory:\", spark.sparkContext.getConf().get(\"spark.driver.memory\", \"Not Set\"))\n",
    "print(\"Number of Partitions:\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('./Data/TMDB_all_movies.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data generator\n",
    "def data_generator(dataframe, chunk_size):\n",
    "    dataframe = dataframe.dropna(subset=['title'])\n",
    "    dataframe = dataframe.coalesce(16)\n",
    "    drop_cols = [\n",
    "        'id', \n",
    "        'vote_average', \n",
    "        'vote_count', \n",
    "        'status', \n",
    "        'release_date', \n",
    "        'revenue', \n",
    "        'runtime', \n",
    "        'budget', \n",
    "        'imdb_id', \n",
    "        'original_language', \n",
    "        'original_title', \n",
    "        'popularity', \n",
    "        'imdb_rating', \n",
    "        'imdb_votes', \n",
    "        'poster_path', \n",
    "        'tagline', \n",
    "        'music_composer', \n",
    "        'director_of_photography'\n",
    "        ]\n",
    "    \n",
    "    dataframe = dataframe.drop(*drop_cols)\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    dataframe = dataframe.withColumn('id', row_number().over(w))\n",
    "    rows = dataframe.count()\n",
    "    \n",
    "    for i in range(0, rows, chunk_size):\n",
    "        print(f\"{i} to {i+chunk_size}\")\n",
    "        chunk = dataframe.filter((col(\"id\")>=i) & (col(\"id\")<(i+chunk_size)))\n",
    "        # Overwrite the existing \"Value\" column with the transformed data\n",
    "        chunk = chunk.withColumn(\"production_companies\", regexp_replace(chunk[\"production_companies\"], \" \", \"\"))\n",
    "        chunk = chunk.withColumn(\"cast\", regexp_replace(regexp_replace(chunk[\"cast\"], \" \", \"\"), \",\", \" \"))\n",
    "        chunk = chunk.withColumn(\"director\", regexp_replace(chunk[\"director\"], \" \", \"\"))\n",
    "        chunk = chunk.withColumn(\"writers\", regexp_replace(chunk[\"writers\"], \" \", \"\"))\n",
    "        chunk = chunk.withColumn(\"producers\", regexp_replace(chunk[\"producers\"], \" \", \"\"))\n",
    "        chunk = chunk.withColumn(\"production_countries\", regexp_replace(chunk[\"production_countries\"], \" \", \"\"))\n",
    "        # chunk = chunk.withColumn(\"genres\", regexp_replace(chunk[\"genres\"], \" \", \"\"))\n",
    "    \n",
    "        chunk = chunk.withColumn(\"Tags\", concat_ws(\" \", chunk[\"title\"], chunk[\"overview\"], chunk[\"genres\"], chunk[\"production_companies\"], chunk[\"production_countries\"], chunk[\"spoken_languages\"], chunk[\"cast\"], chunk[\"director\"], chunk[\"writers\"], chunk[\"producers\"]))\n",
    "\n",
    "        chunk = chunk.select('id', 'title', 'Tags')\n",
    "\n",
    "        chunk = chunk.withColumn(\"Tags\", split(lower(regexp_replace(chunk[\"Tags\"], r\"[^\\p{L}\\s]\", \"\")), \" \"))\n",
    "        chunk.cache()\n",
    "        yield chunk\n",
    "        chunk.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Custom CountVectorizer with minDF, maxDF, and top k\n",
    "def word_occurrence(generator, input_col):\n",
    "    # Step 2.1: Count word occurrences\n",
    "    # word_in_corpus_freq = Counter() # How many times is each word ocurring in the whole corpus\n",
    "    word_in_doc_freq = Counter() # How many documents include each word\n",
    "    \n",
    "    emp_RDD = sc.emptyRDD()\n",
    "    columns = StructType([StructField('id', IntegerType(), False),\n",
    "                          StructField('title', StringType(), False),\n",
    "                          StructField('Tags', ArrayType(StringType()), False),])\n",
    "    df = spark.createDataFrame(data=emp_RDD, schema=columns)\n",
    "    total_docs = 0\n",
    "    for chunk in generator:\n",
    "        df = df.union(chunk)\n",
    "        word_counts = chunk.select(explode(array_distinct(input_col))).rdd \\\n",
    "            .map(lambda x: (x,1)) \\\n",
    "                .reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "        word_counts_collected = dict(word_counts.collect())\n",
    "        # word_in_corpus_freq.update(word_counts_collected)\n",
    "        word_in_doc_freq.update(word_counts_collected)\n",
    "        print(f\"Updated Vocab Length: {len(word_in_doc_freq)}\")\n",
    "    \n",
    "    df_preprocessed = df.repartition(32)\n",
    "\n",
    "    return df_preprocessed, word_in_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply the custom CountVectorizer\n",
    "chunk_size = 100000\n",
    "generator = data_generator(data, chunk_size)\n",
    "vocab_size = 100000\n",
    "data_processed, vocab = word_occurrence(generator, \"Tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed.filter(col(\"title\")==\"Ariel\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vocab(word_counts, docs_count, minDF, maxDF, vocab_size):\n",
    "    # Step 2.2: Apply minDF and maxDF\n",
    "    word_counts = dict(word_counts)\n",
    "    filtered_vocab = {\n",
    "        word.col: count for word, count in word_counts.items()\n",
    "        if ((count / docs_count) >= minDF) and ((count / docs_count) <= maxDF) and (word.col not in stop_words)\n",
    "    }\n",
    "    print(f\"Vocab Length after Filtering: {len(filtered_vocab.keys())}\")\n",
    "    # Step 2.3: Select top k words by frequency\n",
    "    top_vocab = [word for word, count in sorted(filtered_vocab.items(), key=lambda x: -x[1])][:vocab_size]\n",
    "    word_index = {word: idx for idx, word in enumerate(top_vocab)}\n",
    "    \n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrsvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
